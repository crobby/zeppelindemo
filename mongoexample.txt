Be sure to change the mongo config so that it listens on other than 127.0.0.1 (comment out that line in /etc/mongod.conf)

be sure to allow port 27017 in the security and access rules for the master (or wherever mongo is installed)




%dep
z.reset()
z.load("org.mongodb.mongo-hadoop:mongo-hadoop-core:1.4.0")
z.load("org.mongodb:mongodb-driver:3.1.0-rc0")



%pyspark

# set up parameters for reading from MongoDB via Hadoop input format
config = {"mongo.input.uri": "mongodb://localhost:27017/marketdata.minbars"}
inputFormatClassName = "com.mongodb.hadoop.MongoInputFormat"

# these values worked but others might as well
keyClassName = "org.apache.hadoop.io.Text"
valueClassName = "org.apache.hadoop.io.MapWritable"
 
# read the 1-minute bars from MongoDB into Spark RDD format
minBarRawRDD = sc.newAPIHadoopRDD(inputFormatClassName, keyClassName, valueClassName, None, None, config)


# takes the verbose raw structure (with extra metadata) and strips down to just the pricing data
minBarRDD = minBarRawRDD.values()
 
import calendar, time, math
 
dateFormatString = '%Y-%m-%d %H:%M'
 
# sort by time and then group into each bar in 60 minutes
groupedBars = minBarRDD.sortBy(lambda doc: str(doc["Timestamp"])).groupBy(lambda doc: 
    (doc["Symbol"], math.floor(calendar.timegm(time.strptime(doc["Timestamp"], dateFormatString)) / (60*60))))
    
# sort by time and then group into each bar in 1440 (1 day) minutes
dailygroupedBars = minBarRDD.sortBy(lambda doc: str(doc["Timestamp"])).groupBy(lambda doc: 
    (doc["Symbol"], math.floor(calendar.timegm(time.strptime(doc["Timestamp"], dateFormatString)) / (1440*60))))    
 
# define function for looking at each group and pulling out OHLC
# assume each grouping is a tuple of (symbol, seconds since epoch) and a resultIterable of 1-minute OHLC records in the group
 
# write function to take a (tuple, group); iterate through group; and manually pull OHLC
def ohlc(grouping):
    low = sys.maxint
    high = -sys.maxint
    i = 0
    groupKey = grouping[0]
    group = grouping[1]
    for doc in group:
        #take time and open from first bar
        if i == 0:
            openTime = doc["Timestamp"]
            openPrice = doc["Open"]
        #assign min and max from the bar if appropriate
        if doc["Low"] < low:
            low = doc["Low"]
        if doc["High"] > high:
            high = doc["High"]
        i = i + 1           
        # take close of last bar
        if i == len(group):
            close = doc["Close"]
    outputDoc = {"Symbol": groupKey[0], 
        "Timestamp": openTime, 
        "Open": openPrice,
        "High": high,
        "Low": low,
        "Close": close}
    # tried returning [None, outputDoc] and seemed exception earlier
    return (None, outputDoc)


resultRDD = groupedBars.map(ohlc)
dailyresultRDD = dailygroupedBars.map(ohlc)


# configuration for output to MongoDB
config["mongo.output.uri"] = "mongodb://localhost:27017/marketdata.hourbars"
outputFormatClassName = "com.mongodb.hadoop.MongoOutputFormat"
 
# This causes ClassCastException apparently because of an issue in Spark logged as SPARK-5361.  Should write to MongoDB but could not test.
resultRDD.saveAsNewAPIHadoopFile("file:///placeholder", outputFormatClassName, None, None, None, None, config)

config["mongo.output.uri"] = "mongodb://localhost:27017/marketdata.daybars"
outputFormatClassName = "com.mongodb.hadoop.MongoOutputFormat"
dailyresultRDD.saveAsNewAPIHadoopFile("file:///placeholder", outputFormatClassName, None, None, None, None, config)





GENERATE tempTable from mongo data....still needs work getting date magic right

%pyspark
import time
import datetime
from pyspark.sql import Row
simpleRDD = resultRDD.map(lambda r: Row(timestamp=datetime.datetime.strptime(r[1]["Timestamp"], "%Y-%m-%d %H:%M"), symbol=r[1]["Symbol"], high=r[1]["High"], low=r[1]["Low"], close=r[1]["Close"], open=r[1]["Open"]))
sqlContext.createDataFrame(simpleRDD).registerTempTable("newmsftresults")



%pyspark
import time
import datetime
from pyspark.sql import Row
daysimpleRDD = dailyresultRDD.map(lambda r: Row(timestamp=datetime.datetime.strptime(r[1]["Timestamp"], "%Y-%m-%d %H:%M"), symbol=r[1]["Symbol"], high=r[1]["High"], low=r[1]["Low"], close=r[1]["Close"], open=r[1]["Open"]))
sqlContext.createDataFrame(daysimpleRDD).registerTempTable("daymsftresults")


%sql
select timestamp, high from newmsftresults
where timestamp >= '2010-08-01' and timestamp <= '2010-08-02 23:59:59'
order by timestamp asc
limit 200


%sql
select timestamp, high from newmsftresults
where timestamp <= ${endDate='2009-12-01 00:00'}
order by timestamp desc
limit 20

